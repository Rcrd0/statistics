#Week 3


##Hierarchiccal Clustering (part 1)

P2 Find things that are close together -> clustering
* How define close
* How do group things
* How visualize and interpret
* https://scholar.google.com/scholar?hl=en&q=cluster+analysis&btnG=&as_sdt=1%2C21&as_sdtp=

P4 Hierarchical clustering
* Bottom up: find closest two things, group together, find next closest
* Requires: distance (euclidean, correlation, manhattan), merging
* Produces: tree

P6 Dinstances
* Euclidean: http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf
* Manhattan: http://en.wikipedia.org/wiki/Taxicab_geometry


## Hierarchical clustering (part 2)

### P9 example
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1,2,1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))

### P10 Calculate the distance
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame) # distance matrix

### P14 Dendogram
par(mar = c(4,4,2,2))
hClustering <- hclust(distxy)
plot (hClustering)
abline(h = 1.0) # 3 grupos (depende de la altura)

## Hierarchical Clustering (part 3)
### P15 Prettier dendograms
mypclust <- function(hclust, lab = hclust$labels, lab.col = rep(1, length(hclust$labels)), hang = 0.1, ...) {
y <- rep(hclust$height, 2)
x <- as.numeric(hclust$merge)
y <- y[which(x < 0)]
x <- x[which(x < 0)]
y <- y[order(x)]
x <- x[order(x)]
plot(hclust, labels = FALSE, hang = hang, ...)
text(x = x, y = y[cluster$order] - (max(hclust$height) * hang), labels = lab[hclust$order], col = lab.col[hclust$order], str = 90, adj = c(1, 0.5), xpd = NA, ...)
}
* http://sux13.github.io/DataScienceSpCourseNotes/4_EXDATA/Exploratory_Data_Analysis_Course_Notes.html

### P17
Even prettier
* http://gallery.r-enthusiasts.com/RGraphGallery.php?graph=79 (enlace no va)

### P18 Merging
* Complete distance: distancia entre los puntos más alejados de cada cluster
* Average distance: distancia entre el centro de masa de cada cluster

### P19 heatmap()
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1,2,1), each = 4), sd = 0.2)
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
heatmap(dataMatrix)


## K-Means Clustering (part 1)

### P3 - Define close
* Distance: eculidean, correlation, manhattan
* Clustering: fix number of clusters, get centroids, assign to closest centroid, recalculate
* Requires: distance, number of clusters, initial guess for centroids
* Produces: final estimate of centroids, assignment of points to centroids
set.seed(1234)
par(mar= c(0,0,0,0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each =4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)
kmeansObj$cluster

### P12 PLotting
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, cex = 3, lwd = 3)

### P13 Heatmaps
set.seed(1234)
dataMatrix <- as.matrix(dataFrame,)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1,2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(datamatrix):1], yaxt = "n") # orden original
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n") # orden por cluster 

## Dimension reduction (part 1)
### Matrix data
set.seed(12345)
par(mar = rep (0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix) [, nrow(dataMatrix):1])

### Heatmap
par(mar = rep(0.2, 4)
heatmap(dataMatrix)

### Pattern
set.seed(678910)
for (i in 1:40) if(rbinom(1, size = 1, prob = 0.5)) dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0,3), each = 5)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])

### P7 Patterns in rows and columns
* tbr

### P8 Related problems
* Find the set of multivariate variables uncorrelated that explain as much as possible -> statistical + data compress
* Principal Component Analysis
* Singular Value Decomposition (SVD): X = U D t(V); D contains singular values
* Principal Components Anslysis (PCA): substract the mean and divide by sd

## Dimension Reduction (Part 2)
### P10 SVD
svd1 <- svd(dataMatrixOrdered))
* tbr
### P12 Principal components
pca1 <- prcomp(dataMatrixOrder, scale = TRUE)

## Dimension Reduction (part 3)
### Missing values
* imputing: library(impute) # impute.knn

### Face example
svd1 <- svd(scale(faceData))
approx1 <- svd1$u[, 1]   %*% t(svd1$v[, 1]) * svd1$d[1]
approx5 <- svd1$u[, 1:5] %*% diag(svd1$d[1:5] %*% t(svd1$v[, 1:5]) 


## Working with Color (part 1)

## Working with Color (part 2)

## Working with Color (part 3)

## Working with Color (part 4)


